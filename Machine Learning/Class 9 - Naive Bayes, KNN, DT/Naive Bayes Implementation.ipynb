{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification from Scratch\n",
    "\n",
    "In this notebook, we'll implement the Naive Bayes algorithm from scratch. We'll go through each step in detail, explaining the concepts and code to ensure a thorough understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "First, we'll import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "We'll use the Iris dataset for our Naive Bayes implementation. Let's load and inspect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "       species  \n",
      "0  Iris-setosa  \n",
      "1  Iris-setosa  \n",
      "2  Iris-setosa  \n",
      "3  Iris-setosa  \n",
      "4  Iris-setosa  \n"
     ]
    }
   ],
   "source": [
    "# Loading the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                    columns= iris['feature_names'] + ['species'])\n",
    "\n",
    "# Mapping target labels to iris species\n",
    "data['species'] = data['species'].map({0: 'Iris-setosa', 1: 'Iris-versicolor', 2: 'Iris-virginica'})\n",
    "\n",
    "# Displaying the first few rows of the dataset\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "\n",
    "We'll split the dataset into training and testing sets to evaluate our Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "The Naive Bayes classifier is based on Bayes' theorem with the \"naive\" assumption of conditional independence between every pair of features given the value of the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "Bayes' theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. The formula is:\n",
    "\n",
    "\\[\n",
    "P(A|B) = \\frac{P(B|A) \\, P(A)}{P(B)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the posterior probability of class (target) given predictor (attribute).\n",
    "- \\( P(B|A) \\) is the likelihood which is the probability of predictor given class.\n",
    "- \\( P(A) \\) is the prior probability of class.\n",
    "- \\( P(B) \\) is the prior probability of predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Assumption\n",
    "\n",
    "The \"naive\" assumption is that the presence of a particular feature in a class is unrelated to the presence of any other feature. In other words, the features are conditionally independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "For continuous data, we assume that the features follow a Gaussian (normal) distribution. The probability density function of a normal distribution is given by:\n",
    "\n",
    "$$ P(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) $$\n",
    "\n",
    "Where \\( \\mu \\) is the mean and \\( \\sigma \\) is the standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the mean and standard deviation for each feature\n",
    "def summarize_by_class(X, y):\n",
    "    summaries = defaultdict(list)\n",
    "    for label in np.unique(y):\n",
    "        features = X[y == label]\n",
    "        summaries[label] = [(np.mean(feature), np.std(feature)) for feature in zip(*features)]\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Class Probabilities\n",
    "\n",
    "Using the summaries (mean and standard deviation), we can calculate the probability of a data point belonging to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Gaussian probability density function\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = np.exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
    "    return (1 / (np.sqrt(2 * np.pi) * stdev)) * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the class probabilities for a given input\n",
    "def calculate_class_probabilities(summaries, input_vector):\n",
    "    probabilities = {}\n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        probabilities[class_value] = 1\n",
    "        for i in range(len(class_summaries)):\n",
    "            mean, stdev = class_summaries[i]\n",
    "            x = input_vector[i]\n",
    "            probabilities[class_value] *= calculate_probability(x, mean, stdev)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "To make a prediction, we select the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a prediction for a given input\n",
    "def predict(summaries, input_vector):\n",
    "    probabilities = calculate_class_probabilities(summaries, input_vector)\n",
    "    best_label, best_prob = None, -1\n",
    "    for class_value, probability in probabilities.items():\n",
    "        if best_label is None or probability > best_prob:\n",
    "            best_prob = probability\n",
    "            best_label = class_value\n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We'll use the training data to calculate the summaries (mean and standard deviation) for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Iris-setosa': [(4.964516129032259, 0.3346142575455468),\n",
       "              (3.3774193548387097, 0.36957663435023635),\n",
       "              (1.4645161290322577, 0.18236508419487185),\n",
       "              (0.24838709677419357, 0.10737623856189187)],\n",
       "             'Iris-versicolor': [(5.8621621621621625, 0.524714260130706),\n",
       "              (2.724324324324324, 0.29537461821220007),\n",
       "              (4.210810810810811, 0.48922651791278887),\n",
       "              (1.3027027027027025, 0.20333235539247596)],\n",
       "             'Iris-virginica': [(6.559459459459459, 0.6499311645295484),\n",
       "              (2.986486486486486, 0.310328650689888),\n",
       "              (5.545945945945946, 0.5370563383777094),\n",
       "              (2.005405405405405, 0.29311555321809374)]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Naive Bayes model\n",
    "summaries = summarize_by_class(X_train, y_train)\n",
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "We'll use the testing data to evaluate the accuracy of our Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate the accuracy of the model\n",
    "def evaluate_model(summaries, X_test, y_test):\n",
    "    correct = 0\n",
    "    for i in range(len(X_test)):\n",
    "        input_vector = X_test[i]\n",
    "        true_label = y_test[i]\n",
    "        predicted_label = predict(summaries, input_vector)\n",
    "        if predicted_label == true_label:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(X_test)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = evaluate_model(summaries, X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented the Naive Bayes algorithm from scratch. We've covered the following steps:\n",
    "- Understanding Bayes' theorem and the naive assumption.\n",
    "- Calculating the mean and standard deviation for each feature in the training data.\n",
    "- Calculating class probabilities using the Gaussian probability density function.\n",
    "- Making predictions by selecting the class with the highest probability.\n",
    "- Evaluating the accuracy of our model on the testing data.\n",
    "\n",
    "Naive Bayes is a simple yet powerful algorithm that performs well on a variety of tasks, especially those involving text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
