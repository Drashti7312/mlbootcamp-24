{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Python\n",
    "\n",
    "In this notebook, we'll implement logistic regression using only two parameters: the slope (m) and the intercept (c). We'll explain each step in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "First, we'll import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Synthetic Dataset\n",
    "\n",
    "We'll create a simple binary classification dataset using `make_classification` from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Dataset\n",
    "\n",
    "Let's visualize the dataset to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the dataset\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis')\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "\n",
    "The sigmoid function is used to map predictions to probabilities. It is defined as:\n",
    "\n",
    "\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\n",
    "\n",
    "Let's implement the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "We'll initialize our model parameters: the slope (m) and the intercept (c). For simplicity, we'll start with zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model parameters\n",
    "m = np.zeros(X_train.shape[1])  # Slope (weights)\n",
    "c = 0  # Intercept (bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "The logistic regression model is defined as:\n",
    "\n",
    "\\[ \\hat{y} = \\sigma(m \\cdot x + c) \\]\n",
    "\n",
    "Where:\n",
    "- \\( \\hat{y} \\) is the predicted probability\n",
    "- \\( m \\) is the slope (weights)\n",
    "- \\( x \\) is the input feature\n",
    "- \\( c \\) is the intercept (bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "def predict_proba(X, m, c):\n",
    "    return sigmoid(np.dot(X, m) + c)\n",
    "\n",
    "def predict(X, m, c, threshold=0.5):\n",
    "    return predict_proba(X, m, c) >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "We'll use the binary cross-entropy loss (also known as log loss) as our cost function. It is defined as:\n",
    "\n",
    "\\[ J(m, c) = - \\frac{1}{m} \\sum_{i=1}^{m} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function (binary cross-entropy loss)\n",
    "def compute_cost(X, y, m, c):\n",
    "    m = len(y)\n",
    "    h = predict_proba(X, m, c)\n",
    "    cost = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We'll use gradient descent to optimize our model parameters. The gradients of the cost function with respect to the parameters are:\n",
    "\n",
    "\\[ \\frac{\\partial J}{\\partial m} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i) x_i \\]\n",
    "\n",
    "\\[ \\frac{\\partial J}{\\partial c} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i) \\]\n",
    "\n",
    "We'll update the parameters using these gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent function\n",
    "def gradient_descent(X, y, m, c, learning_rate, epochs):\n",
    "    m = len(y)\n",
    "    for _ in range(epochs):\n",
    "        h = predict_proba(X, m, c)\n",
    "        m_grad = (1 / m) * np.dot(X.T, (h - y))\n",
    "        c_grad = (1 / m) * np.sum(h - y)\n",
    "        m -= learning_rate * m_grad\n",
    "        c -= learning_rate * c_grad\n",
    "    return m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Let's train our logistic regression model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Initial parameters\n",
    "m = np.zeros(X_train.shape[1])\n",
    "c = 0\n",
    "\n",
    "# Gradient descent\n",
    "m, c = gradient_descent(X_train, y_train, m, c, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "We'll evaluate the model using accuracy and log loss on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "y_pred_proba = predict_proba(X_test, m, c)\n",
    "y_pred = predict(X_test, m, c)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "loss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Log Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Decision Boundary\n",
    "\n",
    "We'll visualize the decision boundary of our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the decision boundary\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "Z = predict(np.c_[xx.ravel(), yy.ravel()], m, c)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o', cmap='viridis')\n",
    "plt.title('Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented logistic regression using only the slope (m) and intercept (c). We've covered the following steps:\n",
    "\n",
    "1. Generated a synthetic dataset.\n",
    "2. Visualized the dataset.\n",
    "3. Defined the sigmoid function.\n",
    "4. Initialized model parameters.\n",
    "5. Implemented the logistic regression model.\n",
    "6. Defined the cost function.\n",
    "7. Implemented gradient descent.\n",
    "8. Trained the model.\n",
    "9. Evaluated the model.\n",
    "10. Visualized the decision boundary.\n",
    "\n",
    "This should give you a solid understanding of how logistic regression works at a fundamental level."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
